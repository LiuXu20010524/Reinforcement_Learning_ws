{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6f4ac2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import glob\n",
    "import math\n",
    "import os\n",
    "import torchvision\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import mujoco\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "178811ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建actor网络\n",
    "class actor_net(nn.Module):\n",
    "    \n",
    "    def __init__(self,N_S,N_A):\n",
    "        super().__init__()\n",
    "        self.linear1=nn.Linear(N_S,64)\n",
    "        self.linear2=nn.Linear(64,128)\n",
    "        self.linear3=nn.Linear(128,64)\n",
    "        self.mean=nn.Linear(64,N_A)\n",
    "        self.vair=nn.Linear(64,N_A)\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        x=self.linear1(inputs)\n",
    "        x=torch.relu(x)\n",
    "        x=self.linear2(x)\n",
    "        x=torch.relu(x)\n",
    "        x=self.linear3(x)\n",
    "        x=torch.relu(x)\n",
    "        mean=torch.tanh(self.mean(x))\n",
    "        vair=torch.exp(torch.tanh(self.vair(x))-2)\n",
    "        return mean,vair\n",
    "    \n",
    "    def choose_data(self,s):\n",
    "        mu,va=self.forward(s)\n",
    "        Pi=torch.distributions.Normal(mu,va)\n",
    "        return Pi.sample().numpy()\n",
    "#创建critic网路\n",
    "class critic_net(nn.Module):\n",
    "    \n",
    "    def __init__(self,N_S):\n",
    "        super().__init__()\n",
    "        self.linear1=nn.Linear(N_S,64)\n",
    "        self.linear2=nn.Linear(64,128)\n",
    "        self.linear3=nn.Linear(128,64)\n",
    "        self.out=nn.Linear(64,1)\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        x=torch.relu(self.linear1(inputs))\n",
    "        x=torch.relu(self.linear2(x))\n",
    "        x=torch.relu(self.linear3(x))\n",
    "        out=self.out(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6335430",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor=actor_net(27,8)\n",
    "critic=critic_net(27)\n",
    "MSE_loss=torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15f6abb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_opt=torch.optim.Adam(actor.parameters(),lr=0.0001)\n",
    "critic_opt=torch.optim.Adam(critic.parameters(),lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "1ce4e88b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4430/4010929862.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mcount_score\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscount\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32melif\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#如果当前数据的mask=0，代表ant正常允许，加上本次数据的折扣回报后，继续循环\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                 \u001b[0mcount_score\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscount\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#如果当前数据的mask=2，代表遇到了1000步停止的，则加critic预测出来的数乘折扣率\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#学习优化的方向：states归一化怎么实现，计算优势使用GAE，Advantage归一化？（这个不一定有）\n",
    "%matplotlib qt5\n",
    "epoches=2000 #设置训练epoches\n",
    "batch_size=2500#设置batch_size\n",
    "buffer_length=25000#replay_buffer中一次存这些个数据，数据格式：[array(states),array(action),int(reward),int(mask)]\n",
    "discount=0.99#折扣回报率\n",
    "env=gym.make('Ant-v4',healthy_reward=0,ctrl_cost_weight=0)#创建环境\n",
    "observation,info=env.reset()#初始化环境\n",
    "# remember_reward=[]#用来储存每一个episode获得的分数总和\n",
    "count_reward_plot=0#用来计算一个episode获得的总分\n",
    "# plt.figure('Ant',figsize=(15,7))\n",
    "for iter in range(epoches):\n",
    "    replay_buffer=[]#replay_buffer储存容器\n",
    "    #收集数据到memory:\n",
    "    for collect in range(buffer_length):#建立获取经验的循环\n",
    "        action=actor.choose_data(torch.from_numpy(observation).type(torch.FloatTensor))\n",
    "        #调用actor_net类定义的choose_data函数来获取action\n",
    "        observation_, reward, truncated, terminated, info=env.step(list(action))#执行antion\n",
    "        count_reward_plot+=reward#计算episode的reward\n",
    "        if truncated==0 and terminated==0:#判断这一局是否可以继续\n",
    "            mask=0#如果不停止，mask设为0\n",
    "        elif truncated==1:#如果ant失败了，mask设为1\n",
    "            mask=1\n",
    "#             plt.clf()\n",
    "#             plt.plot(remember_reward,'r')\n",
    "#             plt.show()\n",
    "#             plt.pause(0.000001)\n",
    "            remember_reward.append(count_reward_plot)#将本episode获得的分数储存起来\n",
    "            count_reward_plot=0#变量重定为0\n",
    "            observation_,info=env.reset()#环境初始化\n",
    "        elif terminated==1:#如果本次episode步数达到1000，将mask设为2\n",
    "            mask=2\n",
    "#             plt.clf()\n",
    "#             plt.plot(remember_reward,'r')\n",
    "#             plt.show()\n",
    "#             plt.pause(0.000001)\n",
    "            remember_reward.append(count_reward_plot)\n",
    "            count_reward_plot=0\n",
    "            observation_,info=env.reset()\n",
    "        replay_buffer.append([observation,action,reward,mask])#在replay_buffer中存储数据\n",
    "        observation=observation_\n",
    "    #计算advantage，存到count_advantage：\n",
    "    count_TD_target=[]#用来储存每一个数据TD_target的容器\n",
    "    for i in range(len(replay_buffer)):#建立遍历replay_buffer每一个数据的循环，用来计算每一个环境的TD_target\n",
    "        if replay_buffer[i][3]==2:#如果当前这个数据的mask为2（代表本数据是到1000自动结束的），则不计算该数据的TD_target\n",
    "            continue\n",
    "        if i==len(replay_buffer)-1:#如果当前数据是replay_buffer的最后一个，则不计算该数据的TD_target，因为没法计算\n",
    "            continue\n",
    "        count_score=0#该变量用来计算折扣回报加和\n",
    "        #以下用来循环计算某步的return(TD_traget)\n",
    "        for j in range(1000):#因为1000步是一个episode最大步数，所以建立1000的循环\n",
    "            if (i+j)==len(replay_buffer)-1:#如果当前数据是replay_buffer的最后一个数据，则加critic预测出来的数乘折扣率\n",
    "                if replay_buffer[i+j][3]==1:\n",
    "                    count_score+=replay_buffer[i+j][2]*(discount**j)\n",
    "                else:\n",
    "                    count_score+=critic(torch.from_numpy(replay_buffer[i+j][0]).type(torch.FloatTensor)).item() \\\n",
    "                    *(discount**j)\n",
    "                break\n",
    "            if replay_buffer[i+j][3]==1:#如果当前数据的mask=1，代表本次数据ant失败了，则加上这次数据的折扣回报就跳出循环\n",
    "                count_score+=replay_buffer[i+j][2]*(discount**j)\n",
    "                break\n",
    "            elif replay_buffer[i+j][3]==0:#如果当前数据的mask=0，代表ant正常允许，加上本次数据的折扣回报后，继续循环\n",
    "                count_score+=replay_buffer[i+j][2]*(discount**j)\n",
    "            elif replay_buffer[i+j][3]==2:#如果当前数据的mask=2，代表遇到了1000步停止的，则加critic预测出来的数乘折扣率\n",
    "                count_score+=critic(torch.from_numpy(replay_buffer[i+j][0]).type(torch.FloatTensor)).item() \\\n",
    "                *(discount**j)\n",
    "                break\n",
    "        count_TD_target.append([count_score])\n",
    "        #将replay_buffer中每一个数据对应的TD_target保存起来（除了mask=2的，之后要舍弃掉），注意格式，要与其他数据对应\n",
    "    del replay_buffer[-1]#之前没有计算replay_buffer最后一个数据的TD_target，所以要删掉replay_buffer中的最后一个数据\n",
    "    replay_buffer=[item for item in replay_buffer if item[3]!=2]#将mask=2的数据从replay_buffer中剔除\n",
    "    #因为达到1000步自动停止的数据（mask=2），会影响运算，所以需要剔除这些数据，让replay_buffer和count_TD_target每个元素对应\n",
    "    #计算旧策略的log_prob：\n",
    "    states=torch.tensor([item[0] for item in replay_buffer]).type(torch.FloatTensor)\n",
    "    #获取replay_buffer中每个数据的states，单独成一个列表，并转为tensor\n",
    "    actions_=torch.tensor([item[1] for item in replay_buffer]).type(torch.FloatTensor)\n",
    "    #获取replay_buffer中每个数据的actions，单独成一个列表，并转为tensor\n",
    "    old_mu,old_std=actor(states)#用actor网络运算获得每个states对应的旧策略均值和旧策略方差\n",
    "    pi=torch.distributions.Normal(old_mu,old_std)#获得旧策略的正态分布，每个states对应8个分布\n",
    "    old_log_prob=pi.log_prob(actions_).sum(1,keepdim=True)\n",
    "    #.log_prob()函数可以获得某分布取值对应的概率值的log值，sum(1)是将每个states对应的8个分布的log_prob加起来，\n",
    "    #因为log(a*b)=loga+logb，keepdim=True是保持原有的数据格式，即为保持pi原有的数据格式,例如：\n",
    "    #[[1,2,3],[3,4,5]].sum(1,keepdim=True)->[[6],[12]],如果keepdim=False,则输出为[6,12]\n",
    "    #sum(0),sum(1),sum()三者输出不同，sum()将所有的数求合，得出最终一个数，sum(0)和sum(1)则求不同维度的和\n",
    "    #以下开始训练：\n",
    "    for batch_time in range(len(replay_buffer)//batch_size):#建立一个epoch的训练，更新次数与batch_size和buffer长度有关\n",
    "        start_num=batch_size*batch_time#此次batch从第几个数据开始训练\n",
    "        end_num=batch_size*batch_time+batch_size#此次batch结束于哪个数据\n",
    "        states_batch=states[start_num:end_num]#获取在此batch区域内的环境states\n",
    "        action_batch=actions_[start_num:end_num]#获取在此batch区域内的动作action\n",
    "        TD_target_batch=torch.tensor(count_TD_target[start_num:end_num]).type(torch.FloatTensor)\n",
    "        #获取在此batch区域内的TD_target，并转化为tensor格式\n",
    "        batch_value=critic(states_batch)#使用当前critic计算此batch各个states的分数\n",
    "        critic_loss=MSE_loss(batch_value,TD_target_batch)#使用MSE_loss，计算critic的损失，TD算法\n",
    "        critic_opt.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        critic_opt.step()\n",
    "        new_mu,new_div=actor(states_batch)#使用当前的actor计算新策略均值和新策略方差\n",
    "        pi=torch.distributions.Normal(new_mu,new_div)#获得新策略的正态分布，每个states对应8个分布\n",
    "        new_log_pron=pi.log_prob(action_batch).sum(1,keepdim=True)#获得新分布动作采样概率乘积的log值，注意保持维度\n",
    "        ratio=torch.exp(new_log_pron-(old_log_prob[start_num:end_num]).detach())\n",
    "        #计算此batch中，新分布动作采样概率与旧分布动作采样概率的比值，使用相减的原因：log(a/b)=loga-logb，再用exp计算去掉log\n",
    "        actor_loss=-torch.min(ratio*(TD_target_batch-batch_value.detach()),torch.clip(ratio,0.8,1.2)* \\\n",
    "                              (TD_target_batch-batch_value.detach())).mean()\n",
    "        #使用PPO2算法计算actor的损失，最前面加一个-号，因为使用Adam优化器是梯度下降，而PPO算法要求的应该是梯度上升，加一个-号\n",
    "        #就可以使用梯度下降了\n",
    "        actor_opt.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        actor_opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58f1685c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ooo=[[1],[3],[4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a5e76c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "del ooo[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6abc323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [3]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ooo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "d778e715",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(remember_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3267fd21",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mclf()\n\u001b[1;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(env\u001b[38;5;241m.\u001b[39mrender())\n\u001b[0;32m----> 9\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpause\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.00000001\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     11\u001b[0m     actionxxx\u001b[38;5;241m=\u001b[39mactor\u001b[38;5;241m.\u001b[39mchoose_data(torch\u001b[38;5;241m.\u001b[39mfrom_numpy(observation)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mFloatTensor))\n",
      "File \u001b[0;32m~/anaconda3/envs/main_use/lib/python3.10/site-packages/matplotlib/pyplot.py:663\u001b[0m, in \u001b[0;36mpause\u001b[0;34m(interval)\u001b[0m\n\u001b[1;32m    661\u001b[0m         canvas\u001b[38;5;241m.\u001b[39mdraw_idle()\n\u001b[1;32m    662\u001b[0m     show(block\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 663\u001b[0m     \u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_event_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43minterval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    665\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(interval)\n",
      "File \u001b[0;32m~/anaconda3/envs/main_use/lib/python3.10/site-packages/matplotlib/backends/backend_qt.py:412\u001b[0m, in \u001b[0;36mFigureCanvasQT.start_event_loop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    410\u001b[0m     _ \u001b[38;5;241m=\u001b[39m QtCore\u001b[38;5;241m.\u001b[39mQTimer\u001b[38;5;241m.\u001b[39msingleShot(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m), event_loop\u001b[38;5;241m.\u001b[39mquit)\n\u001b[0;32m--> 412\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _maybe_allow_interrupt(event_loop):\n\u001b[1;32m    413\u001b[0m     qt_compat\u001b[38;5;241m.\u001b[39m_exec(event_loop)\n",
      "File \u001b[0;32m~/anaconda3/envs/main_use/lib/python3.10/contextlib.py:142\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m         \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/main_use/lib/python3.10/site-packages/matplotlib/backends/qt_compat.py:230\u001b[0m, in \u001b[0;36m_maybe_allow_interrupt\u001b[0;34m(qapp)\u001b[0m\n\u001b[1;32m    228\u001b[0m signal\u001b[38;5;241m.\u001b[39msignal(signal\u001b[38;5;241m.\u001b[39mSIGINT, old_sigint_handler)\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handler_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 230\u001b[0m     \u001b[43mold_sigint_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhandler_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%matplotlib qt5\n",
    "env=gym.make(\"Ant-v4\",healthy_reward=0,ctrl_cost_weight=0,render_mode='rgb_array')\n",
    "observation,info=env.reset()\n",
    "times=0\n",
    "plt.figure('Ant',figsize=(6,6))\n",
    "while(1):\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render())\n",
    "    plt.pause(0.00000001)\n",
    "    with torch.no_grad():\n",
    "        actionxxx=actor.choose_data(torch.from_numpy(observation).type(torch.FloatTensor))\n",
    "    observation, reward, truncated, terminated, info=env.step(actionxxx)\n",
    "    times+=1\n",
    "    if terminated:\n",
    "        print('end by 1000,times=',times)\n",
    "        print(info)\n",
    "        break\n",
    "    elif truncated:\n",
    "        print('end by fail,times=',times)\n",
    "        print(info)\n",
    "        break\n",
    "env.close()\n",
    "print('end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4478dd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(actor.state_dict(),'./MuJoCo_Antv4_PPO2_actor_state_dict.pth')\n",
    "torch.save(critic.state_dict(),'./MuJoCo_Antv4_PPO2_critic_state_dict.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0007f8cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor.load_state_dict(torch.load('./MuJoCo_Antv4_PPO2_actor_state_dict.pth'))\n",
    "critic.load_state_dict(torch.load('./MuJoCo_Antv4_PPO2_critic_state_dict.pth'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_use",
   "language": "python",
   "name": "main_use"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
