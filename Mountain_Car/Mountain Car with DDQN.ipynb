{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import glob\n",
    "import os\n",
    "import torchvision\n",
    "import gym\n",
    "from pyvirtualdisplay import Display\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DQN=torch.nn.Sequential(nn.Linear(in_features=2,out_features=32,bias=True),\n",
    "                       nn.ReLU(),\n",
    "                       nn.Linear(in_features=32,out_features=64,bias=True),\n",
    "                       nn.ReLU(),\n",
    "                       nn.Linear(in_features=64,out_features=128,bias=True),\n",
    "                       nn.ReLU(),\n",
    "                       nn.Linear(in_features=128,out_features=256,bias=True),\n",
    "                       nn.ReLU(),\n",
    "                       nn.Linear(in_features=256,out_features=512,bias=True),\n",
    "                       nn.ReLU(),\n",
    "                       nn.Linear(in_features=512,out_features=256,bias=True),\n",
    "                       nn.ReLU(),\n",
    "                       nn.Linear(in_features=256,out_features=3,bias=True))\n",
    "targetDQN=copy.deepcopy(DQN)\n",
    "for i,j in zip(DQN.parameters(),targetDQN.parameters()):\n",
    "    (j.data).copy_(i.data)\n",
    "for i in targetDQN.parameters():\n",
    "    i.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=torch.optim.Adam(DQN.parameters(),lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: 1.783791248016597e-05\n",
      "action: 2 reward: 0.0010806258054379052\n",
      "action: 2 reward: 0.005698508403018436\n",
      "action: 2 reward: 0.01631761403984415\n",
      "action: 2 reward: 0.035046742008465984\n",
      "action: 2 reward: 0.06354810603998702\n",
      "action: 2 reward: 0.10296962777763324\n",
      "action: 2 reward: 0.15391852124850353\n",
      "action: 2 reward: 0.21647253589931909\n",
      "action: 2 reward: 0.2902216666054674\n",
      "action: 2 reward: 0.37433214129398584\n",
      "action: 0 reward: 0.45987928046129334\n",
      "action: 0 reward: 0.5424573656737317\n",
      "action: 0 reward: 0.6178415261350558\n",
      "action: 0 reward: 0.682224471291555\n",
      "action: 0 reward: 0.7324014776557446\n",
      "action: 0 reward: 0.7659079837547258\n",
      "action: 0 reward: 0.7811151203583749\n",
      "action: 0 reward: 0.7772881364736994\n",
      "action: 0 reward: 0.7546116727524464\n",
      "action: 0 reward: 0.714183455068403\n",
      "action: 0 reward: 0.6579761096053539\n",
      "action: 0 reward: 0.5887643681215329\n",
      "action: 0 reward: 0.5100133913754326\n",
      "action: 0 reward: 0.42572277309602125\n",
      "action: 0 reward: 0.3402212048313135\n",
      "action: 0 reward: 0.2579086623075706\n",
      "action: 0 reward: 0.1829474587277408\n",
      "action: 0 reward: 0.11890957483075931\n",
      "action: 0 reward: 0.06839770965484393\n",
      "action: 0 reward: 0.03266695198787257\n",
      "action: 0 reward: 0.011285280398410851\n",
      "action: 0 reward: 0.0018783291755800889\n",
      "action: 0 reward: 5.384337981289063e-06\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: 2.3247526922597216e-05\n",
      "action: 2 reward: 0.004323238506303277\n",
      "action: 2 reward: 0.026038263730401398\n",
      "action: 2 reward: 0.07853310652712794\n",
      "action: 2 reward: 0.1735539881987091\n",
      "action: 2 reward: 0.3208645600521052\n",
      "action: 2 reward: 0.5281871053655673\n",
      "action: 2 reward: 0.801413941084246\n",
      "action: 2 reward: 1.145026699864525\n",
      "action: 2 reward: 1.562651664106475\n",
      "action: 2 reward: 2.0576866478519835\n",
      "action: 2 reward: 2.6339501383907225\n",
      "action: 2 reward: 3.296322804570088\n",
      "action: 2 reward: 4.051370604561197\n",
      "action: 2 reward: 4.90795363934822\n",
      "action: 2 reward: 5.877833752551825\n",
      "action: 2 reward: 6.976297909161987\n",
      "action: 2 reward: 100\n",
      "Episode End , steps= 140\n",
      "10174\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: 0.0001112370220387512\n",
      "action: 0 reward: 0.0007896242002422514\n",
      "action: 0 reward: 0.002301533116455981\n",
      "action: 0 reward: 0.004627426524260037\n",
      "action: 0 reward: 0.007534403217596223\n",
      "action: 0 reward: 0.010656266949573973\n",
      "action: 0 reward: 0.013573612490650127\n",
      "action: 0 reward: 0.015887249629742507\n",
      "action: 0 reward: 0.017279626124217472\n",
      "action: 0 reward: 0.017559832626486194\n",
      "action: 0 reward: 0.016689488007403646\n",
      "action: 0 reward: 0.014787844418901889\n",
      "action: 0 reward: 0.012115834445126214\n",
      "action: 0 reward: 0.009039971820566527\n",
      "action: 0 reward: 0.0059784418167100044\n",
      "action: 0 reward: 0.003332862383907737\n",
      "action: 0 reward: 0.0014106307339865334\n",
      "action: 0 reward: 0.0003439765420102419\n",
      "action: 0 reward: 1.2805675064511718e-05\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: 1.1066204237634557e-07\n",
      "action: 2 reward: 0.0014999761858085592\n",
      "action: 2 reward: 0.011175015910611963\n",
      "action: 2 reward: 0.03640641366236644\n",
      "action: 2 reward: 0.08371810337915271\n",
      "action: 2 reward: 0.15849301731832907\n",
      "action: 2 reward: 0.2648346855737552\n",
      "action: 2 reward: 0.4055673045955383\n",
      "action: 2 reward: 0.5823521837085426\n",
      "action: 2 reward: 0.7958893025277808\n",
      "action: 2 reward: 1.0461712958276501\n",
      "action: 2 reward: 1.332760082319213\n",
      "action: 2 reward: 1.6550623487998792\n",
      "action: 2 reward: 2.0125896785662243\n",
      "action: 2 reward: 2.4051933659871727\n",
      "action: 2 reward: 2.833274576367295\n",
      "action: 2 reward: 3.2979698951339014\n",
      "action: 2 reward: 3.8013232577716147\n",
      "action: 2 reward: 4.346448220457763\n",
      "action: 2 reward: 4.937691828410096\n",
      "action: 2 reward: 5.5808151932860985\n",
      "action: 2 reward: 6.283191249560036\n",
      "action: 2 reward: 7.054040840249471\n",
      "action: 2 reward: 100\n",
      "Episode End , steps= 146\n",
      "10320\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: 0.0006955492949953985\n",
      "action: 2 reward: 0.005968872357173618\n",
      "action: 2 reward: 0.020391791756722396\n",
      "action: 2 reward: 0.04802473271193875\n",
      "action: 2 reward: 0.09221307343861194\n",
      "action: 2 reward: 0.1554639147876521\n",
      "action: 2 reward: 0.23940366859839074\n",
      "action: 2 reward: 0.3448071990423759\n",
      "action: 2 reward: 0.47168397192972955\n",
      "action: 2 reward: 0.6194033834784498\n",
      "action: 2 reward: 0.7868415415396874\n",
      "action: 2 reward: 0.9725341482379314\n",
      "action: 2 reward: 1.174823286964288\n",
      "action: 2 reward: 1.3919901141535107\n",
      "action: 2 reward: 1.6223680831327352\n",
      "action: 2 reward: 1.8644362071770133\n",
      "action: 2 reward: 2.116891042150566\n",
      "action: 2 reward: 2.3787016615960788\n",
      "action: 2 reward: 2.649149300950822\n",
      "action: 2 reward: 2.9278549303041763\n",
      "action: 2 reward: 3.214798702652\n",
      "action: 2 reward: 3.5103362658571062\n",
      "action: 2 reward: 3.815209661692932\n",
      "action: 2 reward: 4.130564487798955\n",
      "action: 2 reward: 4.457965813494677\n",
      "action: 2 reward: 4.799424394824097\n",
      "action: 2 reward: 5.157427293423071\n",
      "action: 2 reward: 5.534983207225078\n",
      "action: 2 reward: 5.9356767687370215\n",
      "action: 2 reward: 6.363742669754147\n",
      "action: 2 reward: 6.8241562259779665\n",
      "action: 2 reward: 100\n",
      "Episode End , steps= 84\n",
      "10404\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: 0.0007634851423713177\n",
      "action: 2 reward: 0.006220812845591293\n",
      "action: 2 reward: 0.02089372338985335\n",
      "action: 2 reward: 0.04878881518356324\n",
      "action: 2 reward: 0.09319781985491418\n",
      "action: 2 reward: 0.1565776706266972\n",
      "action: 2 reward: 0.24051032227439625\n",
      "action: 2 reward: 0.3457332814734151\n",
      "action: 2 reward: 0.47222611694270333\n",
      "action: 2 reward: 0.6193352884764871\n",
      "action: 2 reward: 0.7859198972452506\n",
      "action: 2 reward: 0.9705030921894542\n",
      "action: 2 reward: 1.1714172601603674\n",
      "action: 2 reward: 1.3869350228266166\n",
      "action: 2 reward: 1.6153809241441457\n",
      "action: 2 reward: 1.8552228517222982\n",
      "action: 2 reward: 2.1051434445699293\n",
      "action: 2 reward: 2.3640931658796407\n",
      "action: 2 reward: 2.631328868572912\n",
      "action: 2 reward: 2.9064405415427483\n",
      "action: 2 reward: 3.1893698715243217\n",
      "action: 2 reward: 3.4804241349702396\n",
      "action: 2 reward: 3.780287645644378\n",
      "action: 2 reward: 4.090036006577126\n",
      "action: 2 reward: 4.411150191836651\n",
      "action: 2 reward: 4.7455400392194536\n",
      "action: 2 reward: 5.095574452101243\n",
      "action: 2 reward: 5.464119229684348\n",
      "action: 2 reward: 5.854593190664826\n",
      "action: 2 reward: 6.271030547922179\n",
      "action: 2 reward: 6.718171708633722\n",
      "action: 2 reward: 7.2015627044027495\n",
      "action: 2 reward: 100\n",
      "Episode End , steps= 85\n",
      "10489\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: 3.9775610018853415e-09\n",
      "action: 0 reward: 7.119488870510001e-07\n",
      "action: 0 reward: 1.774108523289353e-06\n",
      "action: 0 reward: 1.364616213358767e-06\n",
      "action: 0 reward: 2.0480110695545928e-07\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: 0.0005420152350843126\n",
      "action: 2 reward: 0.005462178689384865\n",
      "action: 2 reward: 0.019624532234198255\n",
      "action: 2 reward: 0.04737907707808975\n",
      "action: 2 reward: 0.09233905905905025\n",
      "action: 2 reward: 0.15724602384096492\n",
      "action: 2 reward: 0.24392107561152093\n",
      "action: 2 reward: 0.35329336539026124\n",
      "action: 2 reward: 0.4854898887281844\n",
      "action: 2 reward: 0.6399678510283915\n",
      "action: 2 reward: 0.8156706165609212\n",
      "action: 2 reward: 1.0111905820804032\n",
      "action: 2 reward: 1.224926005335977\n",
      "action: 2 reward: 1.4552232632906426\n",
      "action: 2 reward: 1.7004984898151712\n",
      "action: 2 reward: 1.9593385603151414\n",
      "action: 2 reward: 2.23058104506274\n",
      "action: 2 reward: 2.5133747102481068\n",
      "action: 2 reward: 2.807225950671462\n",
      "action: 2 reward: 3.112034081579905\n",
      "action: 2 reward: 3.428117591092732\n",
      "action: 2 reward: 3.7562392732225507\n",
      "action: 2 reward: 4.097628713662001\n",
      "action: 2 reward: 4.4540115958947455\n",
      "action: 2 reward: 4.827640101652743\n",
      "action: 2 reward: 5.221339713952168\n",
      "action: 2 reward: 5.638561210129442\n",
      "action: 2 reward: 6.083454030671543\n",
      "action: 2 reward: 6.560955909513591\n",
      "action: 2 reward: 7.076904185462725\n",
      "action: 2 reward: 100\n",
      "Episode End , steps= 128\n",
      "10617\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: 3.958608718885417e-05\n",
      "action: 2 reward: 0.0018701597283150059\n",
      "action: 2 reward: 0.009487999657234734\n",
      "action: 2 reward: 0.026738392035197463\n",
      "action: 2 reward: 0.05693024534717713\n",
      "action: 2 reward: 0.10268201613003747\n",
      "action: 2 reward: 0.16583806054315786\n",
      "action: 2 reward: 0.24745234268716726\n",
      "action: 2 reward: 0.34783029663047405\n",
      "action: 2 reward: 0.4666148852339707\n",
      "action: 2 reward: 0.6029019282261168\n",
      "action: 2 reward: 0.7553696868702293\n",
      "action: 2 reward: 0.922410452417461\n",
      "action: 2 reward: 1.1022545253204727\n",
      "action: 2 reward: 1.2930801389157391\n",
      "action: 2 reward: 1.4931057711529059\n",
      "action: 2 reward: 1.7006642473373206\n",
      "action: 2 reward: 1.9142577235061133\n",
      "action: 2 reward: 2.1325980703338647\n",
      "action: 2 reward: 2.3546319044699766\n",
      "action: 2 reward: 2.5795558728583545\n",
      "action: 2 reward: 2.8068223388919633\n",
      "action: 2 reward: 3.036140406954085\n",
      "action: 2 reward: 3.267473329417551\n",
      "action: 2 reward: 3.5010338989965857\n",
      "action: 2 reward: 3.7372790212486136\n",
      "action: 2 reward: 3.976908136559521\n",
      "action: 2 reward: 4.220863497684645\n",
      "action: 2 reward: 4.470331227126173\n",
      "action: 2 reward: 4.726751712771065\n",
      "action: 2 reward: 4.991832270639551\n",
      "action: 2 reward: 5.267566928521554\n",
      "action: 2 reward: 5.556264136452351\n",
      "action: 2 reward: 5.860583223062468\n",
      "action: 2 reward: 6.183579840416527\n",
      "action: 2 reward: 6.528763266024603\n",
      "action: 2 reward: 6.900169171809568\n",
      "action: 2 reward: 100\n",
      "Episode End , steps= 107\n",
      "10724\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: 2.6147599117320662e-08\n",
      "action: 0 reward: 4.3065804789704825e-08\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: 0.00045095813080411233\n",
      "action: 2 reward: 0.004917147806998647\n",
      "action: 2 reward: 0.01808581392775979\n",
      "action: 2 reward: 0.04416271213279721\n",
      "action: 2 reward: 0.08665080008192225\n",
      "action: 2 reward: 0.14821496959189182\n",
      "action: 2 reward: 0.23062938182633566\n",
      "action: 2 reward: 0.33479937234984913\n",
      "action: 2 reward: 0.4608432422507017\n",
      "action: 2 reward: 0.6082160296588994\n",
      "action: 2 reward: 0.7758573316365631\n",
      "action: 2 reward: 0.9623468516942234\n",
      "action: 2 reward: 1.1660551431932247\n",
      "action: 2 reward: 1.3852804026250396\n",
      "action: 2 reward: 1.6183666626289874\n",
      "action: 2 reward: 1.8638006159086273\n",
      "action: 2 reward: 2.120288774131505\n",
      "action: 2 reward: 2.3868149386811677\n",
      "action: 2 reward: 2.6626829798968075\n",
      "action: 2 reward: 2.947547984355609\n",
      "action: 2 reward: 3.2414384541021644\n",
      "action: 2 reward: 3.544773388306459\n",
      "action: 2 reward: 3.858379381051464\n",
      "action: 2 reward: 4.183505705847028\n",
      "action: 2 reward: 4.521849549242155\n",
      "action: 2 reward: 4.875579745312363\n",
      "action: 2 reward: 5.247377984280323\n",
      "action: 2 reward: 5.64048543990075\n",
      "action: 2 reward: 6.058766413057413\n",
      "action: 2 reward: 6.506790792100585\n",
      "action: 2 reward: 6.989931911818802\n",
      "action: 2 reward: 100\n",
      "Episode End , steps= 105\n",
      "10829\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: 6.1107396997371e-05\n",
      "action: 2 reward: 0.0021153273086002224\n",
      "action: 2 reward: 0.010174667722296749\n",
      "action: 2 reward: 0.028042269861861795\n",
      "action: 2 reward: 0.05897581417016421\n",
      "action: 2 reward: 0.10553819598615496\n",
      "action: 2 reward: 0.16951810600508124\n",
      "action: 2 reward: 0.2519173315612514\n",
      "action: 2 reward: 0.35299541736619194\n",
      "action: 2 reward: 0.4723572894999035\n",
      "action: 2 reward: 0.6090692645458627\n",
      "action: 2 reward: 0.7617884300603105\n",
      "action: 2 reward: 0.9288935151996359\n",
      "action: 2 reward: 1.1086075980811296\n",
      "action: 2 reward: 1.2991067445283697\n",
      "action: 2 reward: 1.4986107979513053\n",
      "action: 2 reward: 1.7054561053875485\n",
      "action: 2 reward: 1.9181494087647732\n",
      "action: 2 reward: 2.135406911739322\n",
      "action: 2 reward: 2.356178829891567\n",
      "action: 2 reward: 2.579662933904449\n",
      "action: 2 reward: 2.8053109591912033\n",
      "action: 2 reward: 3.0328285823174954\n",
      "action: 2 reward: 3.262171014270952\n",
      "action: 2 reward: 3.49354069617526\n",
      "action: 2 reward: 3.7273787732611536\n",
      "action: 2 reward: 3.9643656652993897\n",
      "action: 2 reward: 4.205417631950864\n",
      "action: 2 reward: 4.4516900203744\n",
      "action: 2 reward: 4.704586119439435\n",
      "action: 2 reward: 4.96576736646381\n",
      "action: 2 reward: 5.237174795637949\n",
      "action: 2 reward: 5.521052792329765\n",
      "action: 2 reward: 5.819985072788513\n",
      "action: 2 reward: 6.136938286483884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: 2 reward: 6.475317074412345\n",
      "action: 2 reward: 6.839032706433778\n",
      "action: 2 reward: 7.232585019143473\n",
      "action: 2 reward: 100\n",
      "Episode End , steps= 96\n",
      "10925\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: 0.00047515222373729405\n",
      "action: 2 reward: 0.004708086738236868\n",
      "action: 2 reward: 0.016829878154601387\n",
      "action: 2 reward: 0.040531870406450524\n",
      "action: 2 reward: 0.07887203478217349\n",
      "action: 2 reward: 0.13415312568949286\n",
      "action: 2 reward: 0.20787282046071218\n",
      "action: 2 reward: 0.30073862093097786\n",
      "action: 2 reward: 0.4127358814941464\n",
      "action: 2 reward: 0.5432333225416953\n",
      "action: 2 reward: 0.6911104098870714\n",
      "action: 2 reward: 0.8548927994785278\n",
      "action: 2 reward: 1.032884237909283\n",
      "action: 2 reward: 1.2232867977498958\n",
      "action: 2 reward: 1.4243049393675267\n",
      "action: 2 reward: 1.634230306536708\n",
      "action: 2 reward: 1.8515081225674552\n",
      "action: 2 reward: 2.0747857518547\n",
      "action: 2 reward: 2.302946283401528\n",
      "action: 2 reward: 2.535129512884317\n",
      "action: 2 reward: 2.770744018201216\n",
      "action: 2 reward: 3.0094702983645814\n",
      "action: 2 reward: 3.2512631518927533\n",
      "action: 2 reward: 3.4963491711296797\n",
      "action: 2 reward: 3.745223071640277\n",
      "action: 2 reward: 3.998649311995244\n",
      "action: 2 reward: 4.257664749415202\n",
      "action: 2 reward: 4.523582161713156\n",
      "action: 2 reward: 4.798003406693953\n",
      "action: 2 reward: 5.082838152378062\n",
      "action: 2 reward: 5.380326905489367\n",
      "action: 2 reward: 5.693076302753138\n",
      "action: 2 reward: 6.024102958772621\n",
      "action: 2 reward: 6.376887932533909\n",
      "action: 2 reward: 6.755448516458131\n",
      "action: 2 reward: 7.164422396178633\n",
      "action: 2 reward: 100\n",
      "Episode End , steps= 89\n",
      "11014\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 0 reward: 9.940843003932182e-11\n",
      "action: 0 reward: 5.927265293992248e-06\n",
      "action: 0 reward: 3.125497437188243e-05\n",
      "action: 0 reward: 6.681252037546941e-05\n",
      "action: 0 reward: 9.276848553482303e-05\n",
      "action: 0 reward: 9.445623985552332e-05\n",
      "action: 0 reward: 7.092885214760794e-05\n",
      "action: 0 reward: 3.546468174297411e-05\n",
      "action: 0 reward: 7.977910977784662e-06\n",
      "action: 0 reward: 2.3476437943213993e-08\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: 5.475955812463209e-05\n",
      "action: 2 reward: 0.0024638501672571344\n",
      "action: 2 reward: 0.01239882136742433\n",
      "action: 2 reward: 0.03482251455254591\n",
      "action: 2 reward: 0.074008772578104\n",
      "action: 2 reward: 0.13335767502916743\n",
      "action: 2 reward: 0.21530563942840214\n",
      "action: 2 reward: 0.32132434381625224\n",
      "action: 2 reward: 0.4519942358432143\n",
      "action: 2 reward: 0.6071325252284311\n",
      "action: 2 reward: 0.7859548388563169\n",
      "action: 2 reward: 0.9872511572438668\n",
      "action: 2 reward: 1.2095604352495184\n",
      "action: 2 reward: 1.4513322219913167\n",
      "action: 2 reward: 1.7110693166895732\n",
      "action: 2 reward: 1.987447665382525\n",
      "action: 2 reward: 2.2794142023097903\n",
      "action: 2 reward: 2.586264042895455\n",
      "action: 2 reward: 2.9077031283335546\n",
      "action: 2 reward: 3.2438961471781633\n",
      "action: 2 reward: 3.595509420424008\n",
      "action: 2 reward: 3.9637485942420754\n",
      "action: 2 reward: 4.350399611298185\n",
      "action: 2 reward: 4.757875066135766\n",
      "action: 2 reward: 5.189268832381866\n",
      "action: 2 reward: 5.6484256060349125\n",
      "action: 2 reward: 6.140027656301524\n",
      "action: 2 reward: 6.669704398756428\n",
      "action: 2 reward: 7.2441671266361\n",
      "action: 2 reward: 100\n",
      "Episode End , steps= 104\n",
      "11118\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: 4.7445844123956213e-07\n",
      "action: 0 reward: 1.4887978464063126e-05\n",
      "action: 0 reward: 5.116711100536792e-05\n",
      "action: 0 reward: 9.287352451220222e-05\n",
      "action: 0 reward: 0.00011785539000321531\n",
      "action: 0 reward: 0.0001128697714713475\n",
      "action: 0 reward: 8.054668026125568e-05\n",
      "action: 0 reward: 3.8074288396439584e-05\n",
      "action: 0 reward: 7.712655756243485e-06\n",
      "action: 0 reward: 4.331884300088177e-09\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: 7.472252096370384e-05\n",
      "action: 2 reward: 0.0027325897509905585\n",
      "action: 2 reward: 0.013271699971256596\n",
      "action: 2 reward: 0.0367259630593193\n",
      "action: 2 reward: 0.07741844844318341\n",
      "action: 2 reward: 0.1387780113232538\n",
      "action: 2 reward: 0.22325288668277465\n",
      "action: 2 reward: 0.33231447732646185\n",
      "action: 2 reward: 0.46653636610361116\n",
      "action: 2 reward: 0.6257279170430795\n",
      "action: 2 reward: 0.8091008736777217\n",
      "action: 2 reward: 1.0154497182176745\n",
      "action: 2 reward: 1.2433295790764172\n",
      "action: 2 reward: 1.4912209638379608\n",
      "action: 2 reward: 1.7576743145586802\n",
      "action: 2 reward: 2.041431644567174\n",
      "action: 2 reward: 2.341526245275155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: 2 reward: 2.6573621788628996\n",
      "action: 2 reward: 2.98877782558026\n",
      "action: 2 reward: 3.3360983241709317\n",
      "action: 2 reward: 3.7001800310113726\n",
      "action: 2 reward: 4.082454160637965\n",
      "action: 2 reward: 4.484973780798422\n",
      "action: 2 reward: 4.910464625863059\n",
      "action: 2 reward: 5.362391944416752\n",
      "action: 2 reward: 5.845037631513389\n",
      "action: 2 reward: 6.3636030996562045\n",
      "action: 2 reward: 6.924332132595046\n",
      "action: 2 reward: 100\n",
      "Episode End , steps= 104\n",
      "11222\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: 2.688231641044063e-05\n",
      "action: 2 reward: 0.0019231891935668714\n",
      "action: 2 reward: 0.010378309888182556\n",
      "action: 2 reward: 0.029989220130422704\n",
      "action: 2 reward: 0.06473395514132935\n",
      "action: 2 reward: 0.11779137914508424\n",
      "action: 2 reward: 0.19144686154105486\n",
      "action: 2 reward: 0.28707958111110377\n",
      "action: 2 reward: 0.40521957358745087\n",
      "action: 2 reward: 0.5456569895614832\n",
      "action: 2 reward: 0.707584879771999\n",
      "action: 2 reward: 0.8897573449410647\n",
      "action: 2 reward: 1.0906486235663906\n",
      "action: 2 reward: 1.3086014945050999\n",
      "action: 2 reward: 1.5419590968760664\n",
      "action: 2 reward: 1.789174815683414\n",
      "action: 2 reward: 2.0489024793021544\n",
      "action: 2 reward: 2.3200651846333153\n",
      "action: 2 reward: 2.6019075326092365\n",
      "action: 2 reward: 2.8940350482935813\n",
      "action: 2 reward: 3.1964430387953495\n",
      "action: 2 reward: 3.509539940218971\n",
      "action: 2 reward: 3.8341671646605784\n",
      "action: 2 reward: 4.171621876235169\n",
      "action: 2 reward: 4.523681738477267\n",
      "action: 2 reward: 4.892639223674414\n",
      "action: 2 reward: 5.281341187310627\n",
      "action: 2 reward: 5.693245807848762\n",
      "action: 2 reward: 6.132493324987575\n",
      "action: 2 reward: 6.603993638236648\n",
      "action: 2 reward: 7.113537906660346\n",
      "action: 2 reward: 100\n",
      "Episode End , steps= 84\n",
      "11306\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: -0.8\n",
      "action: 2 reward: 1.7874091315748732e-06\n",
      "action: 0 reward: 1.3425987506161096e-05\n",
      "action: 0 reward: 3.0160809674995862e-05\n",
      "action: 0 reward: 3.9883276105285334e-05\n",
      "action: 0 reward: 3.548797419186561e-05\n",
      "action: 0 reward: 2.0177138342947697e-05\n",
      "action: 0 reward: 5.169847944854914e-06\n",
      "action: 0 reward: 3.071969865580947e-08\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n",
      "action: 0 reward: -0.8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-bed99c000a04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mtarget_score\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargetDQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_num\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlen_mult_step\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen_mult_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscoret\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtarget_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0mhow_many\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#loss.backward会将梯度累加，这里求梯度的平均\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#使用的算法：TD（Q-learning）、Double DQN、Multiple Step、Experience Replay\n",
    "%matplotlib qt5\n",
    "buffer_lengthest=50000\n",
    "buffer_length_to_train=10000\n",
    "minibatch_size=50\n",
    "discount=0.98\n",
    "env=gym.make('MountainCar-v0',render_mode='rgb_array')\n",
    "observation,info=env.reset()\n",
    "#plt.figure('CartPole-v1',figsize=(4,4))\n",
    "update_target_DQN=0\n",
    "alpha_greedy=0.00001\n",
    "steps=0\n",
    "# replay_buffer=[]\n",
    "while(1):\n",
    "    small_buffer=[]#创建小buffer，再整体存入replay_buffer中\n",
    "    #plt.clf()#清除当前图像，必须要有，否则图片会积压住，后来很影响整体代码运行速度\n",
    "    small_buffer.append(list(observation))\n",
    "    #plt.imshow(env.render())#显示图像\n",
    "    #plt.pause(0.00000001)#暂停一小会，必须有\n",
    "    a=random.random()#使用α-greedy算法，先取一个随机数\n",
    "    act=DQN(torch.from_numpy(observation))#DQN运算\n",
    "    if a>alpha_greedy:#如果随机数大于阈值，选DQN分值最大的\n",
    "        action=torch.argmax(act).item()\n",
    "        #scoret=torch.max(act)#获得当前t时刻预测的分数\n",
    "    else:#否则，随机选择动作\n",
    "        action=random.randint(0,2)\n",
    "        #scoret=act[action]#获得当前t时刻预测的分数\n",
    "    small_buffer.append(action)\n",
    "    observation, reward, terminated, truncated, info=env.step(action)#环境执行动作，获得新观测值、奖励以及其他\n",
    "    if observation[0]>-0.4 and observation[0]<0.5:\n",
    "        reward=10*(observation[0]+0.4)**3\n",
    "    elif observation[0]>=0.5:\n",
    "        reward=100\n",
    "    elif observation[0]<=-0.4:\n",
    "        reward=-0.8\n",
    "    small_buffer.append(reward)\n",
    "    small_buffer.append(list(observation))\n",
    "    print('action:',action,'reward:',reward)\n",
    "    steps+=1\n",
    "    #下面是开始训练的代码：\n",
    "    if len(replay_buffer)>=buffer_length_to_train:#当replay_buffer中的数据长度超过buffer_length_to_train的时候才开始训练\n",
    "        if len(replay_buffer)==buffer_length_to_train:\n",
    "            print('Start Train')\n",
    "        how_many=0#在本代码算法结构中，如果在multi_step中遇到自然结束（500步），就舍弃掉这个，how_many就是来计算实际有多少个梯度，用来计算梯度平均\n",
    "        opt.zero_grad()#将DQN梯度清零\n",
    "        for epoch in range(minibatch_size):#建立循环，minibatch_size个\n",
    "            drop_this_epoch=0#标识符，用来判断\n",
    "            end_by_fail=0#标识符，用来判断\n",
    "            target_score=0#TD算法的目标分数，初始化为0\n",
    "            start_num=random.randint(0,len(replay_buffer)-51)#要在replay_buffer中取样本，用随机数生成第一个数的序号\n",
    "            len_mult_step=random.randint(2,50)#训练使用multipe_step,也用随机数生成step距离\n",
    "            for i in range(len_mult_step):\n",
    "                if replay_buffer[start_num+i][4]==1:#杆子倒了，失败了\n",
    "                    len_mult_step=i+1\n",
    "                    end_by_fail=1#标识符\n",
    "                    break\n",
    "                elif replay_buffer[start_num+i][4]==2:#超过500轮，自动停止\n",
    "                    drop_this_epoch=1#标识符，意味舍弃这一组数据，不计算梯度\n",
    "                    break\n",
    "            if drop_this_epoch==1:\n",
    "                continue#如果遇到自然停止的（超过500），跳出这一轮循环，不计算梯度\n",
    "            scoret=(DQN(torch.Tensor(replay_buffer[start_num][0])))[replay_buffer[start_num][1]]#获得t时刻分数\n",
    "            if end_by_fail==1:#如果遇到失败的清空，target_score就是折扣汇报的累加，不需要targetDQN计算\n",
    "                for i in range(len_mult_step):\n",
    "                    target_score+=replay_buffer[start_num+i][2]*pow(discount,i)\n",
    "            else:\n",
    "                for i in range(len_mult_step):\n",
    "                    target_score+=replay_buffer[start_num+i][2]*pow(discount,i)\n",
    "                target_score+=torch.max(targetDQN(torch.Tensor(replay_buffer[start_num+len_mult_step-1][3])))*pow(discount,len_mult_step)\n",
    "            loss=0.5*pow((scoret-target_score),2)\n",
    "            loss.backward()\n",
    "            how_many+=1\n",
    "        for i in DQN.parameters():#loss.backward会将梯度累加，这里求梯度的平均\n",
    "            (i.grad.data).div_(how_many)\n",
    "#             print(i.grad)\n",
    "        opt.step()#优化器执行\n",
    "        #print('optim step successfully')\n",
    "        update_target_DQN+=1\n",
    "        if update_target_DQN==30:#更新targetDQN\n",
    "            update_target_DQN=0\n",
    "            for i,j in zip(DQN.parameters(),targetDQN.parameters()):\n",
    "                j.data.mul_(0.3)\n",
    "                j.data.add_((i.data)*0.7)\n",
    "    ################################################\n",
    "    if not (truncated or terminated):\n",
    "        small_buffer.append(0)\n",
    "        if len(replay_buffer)>=buffer_lengthest:\n",
    "            del replay_buffer[0]#如果replay_buffer的总长度大于阈值（3000），则添加新的时候，删掉最老的（序号为0的）\n",
    "        replay_buffer.append(small_buffer)\n",
    "    elif terminated:#成功到达终点\n",
    "        small_buffer.append(1)\n",
    "        small_buffer[2]+=5000\n",
    "        if len(replay_buffer)>=buffer_lengthest:\n",
    "            del replay_buffer[0]\n",
    "        replay_buffer.append(small_buffer)\n",
    "        print('Episode End , steps=', steps)\n",
    "        steps=0\n",
    "        print(len(replay_buffer))\n",
    "        observation,info=env.reset()\n",
    "    elif truncated:#超过200轮，自动停止\n",
    "        small_buffer.append(2)\n",
    "        if len(replay_buffer)>=buffer_lengthest:\n",
    "            del replay_buffer[0]\n",
    "        replay_buffer.append(small_buffer)\n",
    "        print('Episode End by 200')\n",
    "        steps=0\n",
    "        observation,info=env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 0\n",
      "action 0\n",
      "action 0\n",
      "action 0\n",
      "action 0\n",
      "action 0\n",
      "action 0\n",
      "action 0\n",
      "action 0\n",
      "action 0\n",
      "action 0\n",
      "action 0\n",
      "action 0\n",
      "action 0\n",
      "action 0\n",
      "action 0\n",
      "action 0\n",
      "action 0\n",
      "action 0\n",
      "action 0\n",
      "action 0\n",
      "action 0\n",
      "action 0\n",
      "action 0\n",
      "action 0\n",
      "action 0\n",
      "action 0\n",
      "action 0\n",
      "action 0\n",
      "action 0\n",
      "action 0\n",
      "action 0\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "action 2\n",
      "Finished, steps= 103\n"
     ]
    }
   ],
   "source": [
    "%matplotlib qt5\n",
    "env=gym.make('MountainCar-v0',render_mode='rgb_array')\n",
    "observation, info=env.reset()\n",
    "plt.figure('MountainCar-v0',figsize=(4,4))\n",
    "steps=0\n",
    "for t in range(1000):\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render())\n",
    "    plt.pause(0.00001)\n",
    "    #print(observation)\n",
    "    with torch.no_grad():\n",
    "        action=torch.argmax(DQN(torch.from_numpy(observation))).item()\n",
    "    #print(aa.type())\n",
    "    #b=DQN(aa)\n",
    "    #print(b)\n",
    "    #print(torch.from_numpy(observation))\n",
    "    #action=env.action_space.sample()\n",
    "    #print(action)\n",
    "    observation, reward, terminated, truncated, info=env.step(action)\n",
    "    steps+=1\n",
    "    print('action',action)\n",
    "    #print(observation.type())\n",
    "    if truncated:\n",
    "        print('Finished by 200')\n",
    "        break\n",
    "    elif terminated:\n",
    "        print('Finished, steps=',steps)\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(DQN,'./DDQN_MountainCar_full.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
